# Question 422
## Question
* A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent.
* The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time.
* Which design should a solutions architect recommend to meet these requirements?

## Options
* A.Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.
* B.Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.
* C.Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.
* D.Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

## Answer
* D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

## Explanation
* Option A 沒考慮的原因是 NLB 主要是處理 TCP/UDP 流量，但 User 發的 API 請求應該是 HTTP/HTTPS 的，所以不太適合。(Note: GPT 指出 Lambda 不能使用在 NLB)
* Option B 
  * App Mesh is overkill for this use case.
  * ALB is better for synchronous traffic, not asynchronous batch processing.
* Option C
  * Lambda has a hard limit of 10 GB RAM and 15 min timeout.
  * Downloading 1 GB per invocation is inefficient.
  * Cold start times will be high and may not scale well for large, stateful ML models.

* 選的時候只有考慮 Option C 和 Option D